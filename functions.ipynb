{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "func.py",
      "provenance": [],
      "authorship_tag": "ABX9TyOphSmPQvBJtxAuUT5fSrw1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mridulbiswas402/Bankloan/blob/master/functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msuudG07qLLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4XUdcL5IXx0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this function take dataframe as input it is to print description shape info of dataframe and returns list of contineous and discrete attribute name\n",
        "def brief_info(df):\n",
        "    discols=[]\n",
        "    conticols=[]\n",
        "    print(df.info())\n",
        "    print(df.shape)\n",
        "    print('Discrete columns are:')\n",
        "    for cols in df.columns:\n",
        "        if (df[cols].dtypes)=='int64':\n",
        "            discols=discols+[cols]\n",
        "    print(discols)       \n",
        "    print('\\n__________________________')\n",
        "    print('Continous Columns are:')\n",
        "    for cols in df.columns:\n",
        "        if (df[cols].dtypes)!='int64':\n",
        "            conticols=conticols+[cols]\n",
        "    print(conticols)       \n",
        "    return df.describe()\n",
        "\n",
        "# this is to value_counts() of all the discrete columns\n",
        "def valuecount(df,columnlist):\n",
        "  for i in columnlist:\n",
        "    print(\"----------\",i,\"-----------\")\n",
        "    print(df[i].value_counts())\n",
        "\n",
        "# this to count plot of all discrete column in the dataframe\n",
        "def showcount(df,dislist,target):\n",
        "    l1=dislist\n",
        "    print(\"------ Bivariate ------\")\n",
        "    for i in l1:\n",
        "        sns.countplot(x=i,hue=target,data=df)\n",
        "        plt.show()\n",
        "    print(\"------ Univariate -----\")  \n",
        "    for i in l1:\n",
        "        sns.countplot(data[i])\n",
        "        plt.show()\n",
        "\n",
        "# it shows the distribution of contineous columns\n",
        "def showdist(dislist,df,target):\n",
        "    l1=dislist\n",
        "    print(\"---------- Bivariate ----------\")\n",
        "    for i in l1:\n",
        "        fig=sns.FacetGrid(data=df,hue=target)\n",
        "        fig.map(sns.kdeplot,i)\n",
        "        fig.add_legend()\n",
        "        plt.show()\n",
        "    print(\"---------- Univariate ----------\") \n",
        "    for i in l1:\n",
        "        sns.distplot(df[i])\n",
        "        plt.show()\n",
        "# the function is to plot boxplot of all numerical attribute wrt categorical attribute\n",
        "def showboxplot(conti,disc,data,target):\n",
        "  for i in conti:\n",
        "    print(\"------------ wrt \",i,\"-------------\")\n",
        "    for j in disc: \n",
        "      sns.boxplot(x=j,y=i,hue=target,data=data)\n",
        "      plt.show()\n",
        "# this is to plot scatterplot all attribute wrt target attri      \n",
        "def showscatterplot(dcol,ccol,data,target):\n",
        "  print(\"---------- wrt \",target,\"-----------\")\n",
        "  for i in ccol+dcol:  \n",
        "    fig,ax = plt.subplots(figsize=(7,5))\n",
        "    sns.scatterplot(x=i, y=target,hue=target, data=data)\n",
        "    plt.show()\n",
        "  print(\"------ among contineous columns ------\")  \n",
        "  for i in ccol:\n",
        "    for j in ccol:\n",
        "      fig,ax = plt.subplots(figsize=(7,5))\n",
        "      sns.scatterplot(x=i, y=j,hue=target, data=data)\n",
        "      plt.show()\n",
        "# this function is transform the skewed attribute to non skewed normalized attribute    \n",
        "def normalize(col,data):\n",
        "  trnsdata = power_transform(data[col], method='yeo-johnson',standardize=True)\n",
        "  trnsdata = pd.DataFrame(data=trnsdata,columns=col)\n",
        "  data = data.drop(col,axis=1)\n",
        "  data = pd.concat([data,trnsdata],axis=1)\n",
        "  return data\n",
        "\n",
        "# this function is to plot the roc_auc curve \n",
        "def Auc_curve(model,Xtest,ytest,ypredicted):\n",
        "    #import sklearn.metrics as metrics\n",
        "    probs = model.predict_proba(Xtest)\n",
        "    preds = probs[:,1]\n",
        "    fpr, tpr, threshold = metrics.roc_curve(ytest, ypredicted)\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "    # method I: plt\n",
        "    #import matplotlib.pyplot as plt\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "    plt.legend(loc = 'lower right')\n",
        "    plt.plot([0, 1], [0, 1],'r--')\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.show()\n",
        "\n",
        "# this function is to test different model on the given data and view different scores for classification problem\n",
        "def modeltest(Xtrain,Xtest,ytrain,ytest,model):\n",
        "        model.fit(Xtrain, ytrain)\n",
        "        ypred = model.predict(Xtest)\n",
        "        print(\"recall :\",metrics.recall_score(ytest,ypred))\n",
        "        print(\"acc :\",metrics.accuracy_score(ytest,ypred))\n",
        "        print(\"roc :\",metrics.roc_auc_score(ytest,ypred))\n",
        "        print(\"f1 :\",metrics.f1_score(ytest,ypred))\n",
        "        print(\"precision:\",metrics.precision_score(ytest,ypred))\n",
        "        Auc_curve(model,Xtest,ytest,ypred)\n",
        "        matrix=confusion_matrix(ytest, ypred)\n",
        "        print(matrix)\n",
        "\n",
        "# the function Most_imp_feature() is for feature selection it implements 1) mutual_info_classif() 2) Randomforestclassifier() 3) REF()\n",
        "# and output a list of names of column which are important common columns in all  \n",
        "def Intersection(lst1, lst2): \n",
        "    return list(set(lst1) & set(lst2))\n",
        "def Most_imp_feature(Xtrain,ytrain):\n",
        "    mi=feature_selection.mutual_info_classif(Xtrain,ytrain)\n",
        "    miser=pd.Series(mi)\n",
        "    miser.index=Xtrain.columns.values\n",
        "    m=miser.sort_values(ascending=False)\n",
        "    robj=ensemble.RandomForestClassifier(n_estimators=200)\n",
        "    robj.fit(Xtrain,ytrain)\n",
        "    ser1=pd.Series(robj.feature_importances_)\n",
        "    ser1.index=Xtrain.columns.values\n",
        "    r=ser1.sort_values(ascending=False)\n",
        "    rfeobj=feature_selection.RFE(estimator=linear_model.LogisticRegression(C=100,penalty=\"l2\"),n_features_to_select=10)\n",
        "    a=rfeobj.fit_transform(Xtrain,ytrain)\n",
        "    f=list(Xtrain.columns[rfeobj.get_support()])\n",
        "    lst1=[]\n",
        "    for i in range(0,10):\n",
        "        lst1=lst1+[m.index[i]]\n",
        "    lst2=[]\n",
        "    for i in range(0,10):\n",
        "        lst2=lst2+[r.index[i]]\n",
        "    \n",
        "    intsec=Intersection(lst1, lst2)\n",
        "    \n",
        "    impfeature=Intersection(intsec, f)\n",
        "    \n",
        "    return impfeature\n",
        "\n",
        "# this function is also for feature selection which implements chi2 test and outputs a list of features\n",
        "def Chi2(Xtraindis,ytrain,dis):\n",
        "    chiarr,parr=feature_selection.chi2(Xtraindis,ytrain)      # chi2 test for discrete values\n",
        "    cols=dis\n",
        "    ser=pd.Series(chiarr)\n",
        "    ser.index=cols\n",
        "    ch=ser.sort_values(ascending=False)\n",
        "    lst1=[]\n",
        "    for i in range(int(len(cols)/2)):\n",
        "            lst1=lst1+[ch.index[i]]\n",
        "    return lst1\n",
        "\n",
        "# this function tests the given data on 4 different model 1) logistic regression 2)Randomforest 3)KNN 4) Naive bayes and outputs the performance score  of the\n",
        "# models so that we can compare the results across the models.\n",
        "def modelstats(Xtrain,Xtest,ytrain,ytest):\n",
        "    stats=[]\n",
        "    modelnames=[\"LR\",\"Randomforest\",\"KNN\",\"NB\"]\n",
        "    models=list()\n",
        "    models.append(linear_model.LogisticRegression())\n",
        "    models.append(ensemble.RandomForestClassifier())\n",
        "    models.append(neighbors.KNeighborsClassifier())\n",
        "    models.append(naive_bayes.GaussianNB())\n",
        "    for name,model in zip(modelnames,models):\n",
        "        if name==\"KNN\":\n",
        "            k=[l for l in range(5,17,2)]\n",
        "            grid={\"n_neighbors\":k}\n",
        "            grid_obj = GridSearchCV(estimator=model,param_grid=grid,scoring=\"f1\")\n",
        "            grid_fit =grid_obj.fit(Xtrain,ytrain)\n",
        "            model = grid_fit.best_estimator_\n",
        "            model.fit(Xtrain,ytrain)\n",
        "            name=name+\"(\"+str(grid_fit.best_params_[\"n_neighbors\"])+\")\"\n",
        "            print(grid_fit.best_params_)\n",
        "        else:\n",
        "            model.fit(Xtrain,ytrain)\n",
        "        trainprediction=model.predict(Xtrain)\n",
        "        testprediction=model.predict(Xtest)\n",
        "        scores=list()\n",
        "        scores.append(name+\"-train\")\n",
        "        scores.append(metrics.accuracy_score(ytrain,trainprediction))\n",
        "        scores.append(metrics.f1_score(ytrain,trainprediction))\n",
        "        scores.append(metrics.precision_score(ytrain,trainprediction))\n",
        "        scores.append(metrics.recall_score(ytrain,trainprediction))\n",
        "        scores.append(metrics.roc_auc_score(ytrain,trainprediction))\n",
        "        stats.append(scores)\n",
        "        scores=list()\n",
        "        scores.append(name+\"-test\")\n",
        "        scores.append(metrics.accuracy_score(ytest,testprediction))\n",
        "        scores.append(metrics.f1_score(ytest,testprediction))\n",
        "        scores.append(metrics.precision_score(ytest,testprediction))\n",
        "        scores.append(metrics.recall_score(ytest,testprediction))\n",
        "        scores.append(metrics.roc_auc_score(ytest,testprediction))\n",
        "        stats.append(scores)\n",
        "    \n",
        "    colnames=[\"MODELNAME\",\"ACCURACY\",\"f1\",\"PRECISION\",\"RECALL\",\"AUC\"]\n",
        "    return pd.DataFrame(stats,columns=colnames),pd.crosstab(ytest,testprediction)\n",
        "\n",
        "# this function does the stratified splitting of the data \n",
        "def splitdata(df,target,ts):\n",
        "  split = StratifiedShuffleSplit(n_splits=1, test_size=ts, random_state=42)\n",
        "  for train_index, valid_index in split.split(df, df[target]):\n",
        "          train = df.loc[train_index]\n",
        "          valid = df.loc[valid_index]\n",
        "  train=train.reset_index(drop=True) \n",
        "  valid=valid.reset_index(drop=True)       \n",
        "  return train,valid"
      ],
      "execution_count": 1,
      "outputs": []
    }
  ]
}